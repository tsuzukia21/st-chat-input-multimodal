# Demo

![Demo](./demo.gif)

# Streamlit Multimodal Chat Input

Streamlitã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã€ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€éŸ³å£°å…¥åŠ›ãŒå¯èƒ½ã§ã™ã€‚

> **æ³¨æ„**: éŸ³å£°æ©Ÿèƒ½ã¨ç”»åƒæ©Ÿèƒ½ã¯HTTPSç’°å¢ƒã¾ãŸã¯localhostç’°å¢ƒã§ã®å‹•ä½œãŒå¿…è¦ã§ã™ã€‚

## ç‰¹å¾´

- ğŸ“ **ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›**: st.chat_inputã¨åŒæ§˜ã®æ“ä½œæ€§
- ğŸ–¼ï¸ **ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: jpg, png, gif, webpå¯¾å¿œ
- ğŸ¤ **éŸ³å£°å…¥åŠ›**: Web Speech API / OpenAI Whisper APIå¯¾å¿œ
- ğŸ¨ **Streamlitæ¨™æº–ãƒ†ãƒ¼ãƒ**: å®Œå…¨å¯¾å¿œãƒ‡ã‚¶ã‚¤ãƒ³
- ğŸ”„ **Drag & Drop**: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å¯¾å¿œ
- âŒ¨ï¸ **Ctrl+V**: ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã‹ã‚‰ã®ç”»åƒè²¼ã‚Šä»˜ã‘
- âš™ï¸ **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½**: è±Šå¯Œãªè¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install st-chat-input-multimodal
```

## åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```python
import streamlit as st
from st_chat_input_multimodal import multimodal_chat_input

# åŸºæœ¬çš„ãªä½¿ç”¨æ³•
result = multimodal_chat_input()

if result:
    # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
    if result['text']:
        st.write(f"ãƒ†ã‚­ã‚¹ãƒˆ: {result['text']}")
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
    if result['files']:
        for file in result['files']:
            import base64
            base64_data = file['data'].split(',')[1]
            image_bytes = base64.b64decode(base64_data)
            st.image(image_bytes, caption=file['name'])
    
    # éŸ³å£°å…¥åŠ›ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    if result.get('audio_metadata'):
        st.write(f"éŸ³å£°å…¥åŠ›ä½¿ç”¨: {result['audio_metadata']['used_voice_input']}")
```

## é«˜åº¦ãªä½¿ç”¨æ–¹æ³•

### éŸ³å£°å…¥åŠ›æ©Ÿèƒ½

```python
# éŸ³å£°å…¥åŠ›ã‚’æœ‰åŠ¹åŒ–
result = multimodal_chat_input(
    enable_voice_input=True,
    voice_recognition_method="web_speech",  # or "openai_whisper"
    voice_language="ja-JP",
    max_recording_time=60
)

# OpenAI Whisper APIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
result = multimodal_chat_input(
    enable_voice_input=True,
    voice_recognition_method="openai_whisper",
    openai_api_key="sk-your-api-key",
    voice_language="ja-JP"
)
```

### ã‚«ã‚¹ã‚¿ãƒ è¨­å®š

```python
result = multimodal_chat_input(
    placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
    max_chars=500,
    accepted_file_types=["jpg", "png", "gif", "webp"],
    max_file_size_mb=10,
    disabled=False,
    key="custom_chat_input"
)
```

### Chatã§ã®ä½¿ç”¨æ–¹æ³•


```python
import streamlit as st
import base64
from st_chat_input_multimodal import multimodal_chat_input

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ãƒ‡ãƒ¢",
    page_icon="ğŸ’¬",
    layout="wide"
)

st.subheader("ğŸ’­ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ãƒ‡ãƒ¢")
st.markdown("éŸ³å£°å…¥åŠ›ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ä»˜ãã®ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¾ã™ã€‚")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç®¡ç†
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¥åŠ›
chat_result = multimodal_chat_input(
    placeholder="ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
    enable_voice_input=True,  # ãƒãƒ£ãƒƒãƒˆã§ã‚‚éŸ³å£°å…¥åŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹
    key="chat_input"
)
if chat_result:
    st.session_state.chat_history.append(chat_result)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
if st.session_state.chat_history:
    for i, message in enumerate(st.session_state.chat_history):
        with st.chat_message("user"):
            if message.get("text"):
                st.write(message["text"])
            
            if message.get("files"):
                for file in message["files"]:
                    try:
                        base64_data = file['data'].split(',')[1] if ',' in file['data'] else file['data']
                        image_bytes = base64.b64decode(base64_data)
                        st.image(image_bytes, caption=file['name'], width=200)
                    except:
                        st.write(f"ğŸ“ {file['name']}")
            
            # éŸ³å£°å…¥åŠ›æƒ…å ±ã‚’è¡¨ç¤º
            if message.get("audio_metadata") and message["audio_metadata"]["used_voice_input"]:
                st.caption(f"ğŸ¤ Voice input ({message['audio_metadata']['transcription_method']})")


# å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
if st.button("å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.chat_history = []
    st.rerun()

```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License

## ä½œè€…

tsuzukia21
